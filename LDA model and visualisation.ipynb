{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a LDA model on sample text and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = '/Users/mz/Desktop/text.txt'\n",
    "with open(fname) as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content = [x.strip() for x in content] \n",
    "doc_complete = content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word frequency no need to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(doc_complete)\n",
    "freq = np.ravel(X.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define stopwords and normalize corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation) \n",
    "stop = set(stopwords.words('english')+['et','al,','al.,','al'])\n",
    "lemma = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean document from stopwords and puctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_clean = [clean(doc).split() for doc in doc_complete]   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#remove high or low frequency words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high_fre = 25; low_fre = 1\n",
    "all_tokens = sum(doc_clean, [])\n",
    "tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) <= low_fre or all_tokens.count(word)>=high_fre)\n",
    "doc_clean = [[word for word in text if word not in tokens_once]for text in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(69 unique tokens: ['small', 'participant', 'first', 'klingberg', 'subprocesses']...)\n"
     ]
    }
   ],
   "source": [
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "print(dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=4, id2word = dictionary, passes=100)\n",
    "\n",
    "print(ldamodel.print_topics(num_topics=2, num_words=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el617915012180328465824439\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el617915012180328465824439_data = {\"R\": 30, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"mdsDat\": {\"cluster\": [1, 1, 1, 1], \"Freq\": [35.11263158987756, 31.29617174405663, 26.56480035540999, 7.026396310655832], \"topics\": [1, 2, 3, 4], \"x\": [0.10893996793350941, -0.031038027637198057, -0.09100361762073689, 0.013101677324425503], \"y\": [0.004042253821814865, -0.0907440786047068, 0.04219592822277518, 0.04450589656011678]}, \"tinfo\": {\"Term\": [\"task\", \"effect\", \"feature\", \"make\", \"test\", \"untrained\", \"also\", \"framework\", \"algorithm\", \"model\", \"data\", \"study\", \"transfer\", \"result\", \"size\", \"although\", \"small\", \"training\", \"would\", \"popular\", \"2007\", \"population\", \"provide\", \"certain\", \"testing\", \"melbylervag\", \"well\", \"wm\", \"one\", \"could\", \"construct\", \"nback\", \"different\", \"could\", \"participant\", \"issue\", \"adopted\", \"one\", \"task\", \"trained\", \"measure\", \"untrained\", \"test\", \"make\", \"feature\", \"wm\", \"interest\", \"procedure\", \"two\", \"first\", \"jaeggi\", \"reported\", \"use\", \"intelligence\", \"holmes\", \"subprocesses\", \"set\", \"shared\", \"hypothesis\", \"following\", \"transfer\", \"result\", \"training\", \"cognitive\", \"ability\", \"would\", \"popular\", \"2007\", \"population\", \"provide\", \"certain\", \"testing\", \"field\", \"also\", \"wm\", \"outcome\", \"interest\", \"training\", \"hulme2013\", \"evidence\", \"reading\", \"astle\", \"attention\", \"intensive\", \"benefit\", \"reliable\", \"klingberg\", \"cognitive\", \"gathercole\", \"researcher\", \"following\", \"hypothesis\", \"shared\", \"set\", \"subprocesses\", \"effect\", \"transfer\", \"task\", \"group\", \"ability\", \"size\", \"although\", \"small\", \"melbylervag\", \"well\", \"effect\", \"result\", \"algorithm\", \"data\", \"model\", \"2008\", \"klingberg\", \"reliable\", \"benefit\", \"intensive\", \"attention\", \"reading\", \"astle\", \"evidence\", \"hulme2013\", \"holmes\", \"intelligence\", \"reported\", \"jaeggi\", \"use\", \"first\", \"two\", \"procedure\", \"study\", \"cognitive\", \"transfer\", \"training\", \"group\", \"ability\", \"outcome\", \"field\", \"algorithm\", \"model\", \"data\", \"also\", \"feature\", \"make\", \"test\", \"untrained\", \"framework\", \"study\", \"effect\", \"transfer\", \"task\", \"size\", \"small\", \"although\", \"reliable\", \"klingberg\", \"reading\", \"astle\", \"benefit\", \"intensive\", \"hulme2013\", \"attention\", \"evidence\", \"first\", \"two\", \"procedure\", \"holmes\", \"intelligence\", \"training\", \"shared\", \"hypothesis\", \"2008\", \"group\", \"cognitive\", \"well\", \"trained\", \"outcome\", \"set\", \"subprocesses\", \"researcher\", \"gathercole\", \"ability\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.7028, -3.7028, -3.7028, -3.7028, -3.7028, -3.7028, -3.7028, -3.7028, -2.1852, -3.3343, -3.7027, -4.2917, -4.2917, -4.2917, -4.2917, -3.3352, -3.7028, -4.2898, -4.2898, -4.29, -4.2907, -4.2907, -4.2907, -4.2907, -4.2909, -4.2903, -4.2903, -4.2904, -4.2904, -4.2906, -3.066, -3.7032, -2.8552, -3.7034, -4.2908, -3.6107, -3.6107, -3.6107, -3.6107, -3.6107, -3.6107, -3.6107, -3.6107, -4.1989, -3.2429, -3.6105, -3.6107, -2.3105, -4.1986, -4.1986, -4.1986, -4.1986, -4.1986, -4.1986, -4.1986, -4.199, -4.199, -2.9741, -4.1983, -4.1983, -4.1985, -4.1987, -4.1987, -4.1988, -4.1988, -3.6124, -3.6126, -3.6145, -4.1984, -4.1986, -3.4829, -3.4829, -3.4829, -3.4826, -3.4828, -2.8458, -3.1149, -4.0741, -4.0741, -4.0741, -3.4818, -4.0703, -4.0703, -4.0706, -4.0706, -4.0706, -4.0706, -4.0706, -4.0706, -4.0706, -4.0704, -4.0705, -4.0705, -4.0705, -4.0705, -4.0713, -4.0715, -4.0715, -3.482, -3.1156, -3.115, -2.8475, -4.0703, -4.0704, -4.0706, -4.0708, -3.216, -3.216, -3.216, -3.2189, -3.2182, -3.2182, -3.2182, -3.2182, -3.217, -3.2188, -3.2202, -3.2192, -3.2223, -4.8287, -4.8287, -4.8287, -4.8285, -4.8285, -4.8286, -4.8286, -4.8286, -4.8286, -4.8286, -4.8286, -4.8286, -4.8283, -4.8283, -4.8283, -4.8285, -4.8286, -3.2242, -4.8285, -4.8285, -4.8285, -4.8285, -4.8285, -4.8285, -4.8285, -4.8285, -4.8285, -4.8285, -4.8285, -4.8285, -4.8285], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.8011, 0.8011, 0.8011, 0.8011, 0.8011, 0.8011, 0.8011, 0.8011, 0.7786, 0.6463, 0.5177, 0.3687, 0.3687, 0.3687, 0.3687, 0.3071, 0.2825, 0.2342, 0.2342, 0.2341, 0.2333, 0.2333, 0.2333, 0.2333, 0.2332, 0.2216, 0.2216, 0.2214, 0.2214, 0.2212, 0.1678, 0.116, -0.1881, -0.4062, -0.0644, 0.9091, 0.9091, 0.9091, 0.9091, 0.9091, 0.9091, 0.9091, 0.6217, 0.4705, 0.3994, 0.3868, 0.3747, 0.3566, 0.3335, 0.3335, 0.3335, 0.3335, 0.3335, 0.3335, 0.3335, 0.3332, 0.3332, 0.3231, 0.3136, 0.3136, 0.3134, 0.3132, 0.3132, 0.313, 0.313, -0.0241, -0.3788, -0.6507, 0.028, 0.0278, 1.0617, 1.0617, 1.0617, 0.7591, 0.7588, 0.7425, 0.7043, 0.6102, 0.6102, 0.6102, 0.5228, 0.4619, 0.4619, 0.4616, 0.4616, 0.4616, 0.4616, 0.4616, 0.4616, 0.4616, 0.4537, 0.4535, 0.4535, 0.4535, 0.4535, 0.4527, 0.4526, 0.4526, 0.4064, 0.1816, 0.1188, -0.1804, 0.1561, 0.156, -0.0733, 0.1616, 1.4683, 1.4683, 1.4683, 1.4506, 1.4421, 1.4421, 1.4421, 1.4421, 1.1089, 0.6696, 0.368, 0.0145, -0.2585, -0.284, -0.284, -0.284, -0.2964, -0.2964, -0.2964, -0.2964, -0.2964, -0.2964, -0.2964, -0.2964, -0.2964, -0.3042, -0.3043, -0.3043, -0.3045, -0.3045, -0.5571, -0.3166, -0.3166, -0.8239, -0.6021, -1.5313, -0.5868, -0.8479, -0.8312, -0.3167, -0.3167, -0.3167, -0.3167, -0.6021], \"Freq\": [10.0, 5.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 4.0, 8.0, 4.0, 2.0, 2.0, 2.0, 14.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 5.0, 2.0, 2.0, 1.817996008059481, 1.817996008042622, 1.8179960078511401, 1.817987902033141, 1.817987902029041, 1.817987902024197, 1.8179879019966223, 1.8179797944015255, 8.292051880355162, 2.6278426117058737, 1.8181571500003784, 1.008876524066867, 1.0088765240141626, 1.0088765238436888, 1.0088765236562476, 2.625661348142531, 1.8178810589474605, 1.0107217646278925, 1.0107217645927373, 1.0105477208485394, 1.0098237550889835, 1.0098237550772258, 1.0098237550362332, 1.0098237549850109, 1.0096497096145294, 1.0102880708892206, 1.0102880708729494, 1.010107518531169, 1.0101075184180854, 1.009929519829072, 3.436780074991616, 1.8172070235630309, 4.242996797552564, 1.8168602614640517, 1.00976966137683, 1.7766023989907909, 1.7766023985851918, 1.7766023983353443, 1.776602398211219, 1.7765911972263648, 1.7765911971800719, 1.7765799932480766, 1.77671414423379, 0.9865842927475035, 2.566557726039661, 1.7770643586523402, 1.776724850663828, 6.5201073246568555, 0.9868833369511739, 0.9868833369288175, 0.9868833365735646, 0.9868833365545643, 0.9868833364709734, 0.9868833362923293, 0.9868833357238717, 0.9865643983871626, 0.9865643982637485, 3.357944752587474, 0.9872140025570628, 0.9872140024007221, 0.9870522328292156, 0.9868450994107358, 0.9868450992984623, 0.9866833257863177, 0.9866833257809013, 1.7737153217838573, 1.7733440047942286, 1.769866260738326, 0.9871091337538048, 0.9869473410715353, 1.7136279899527422, 1.7136279891706716, 1.7136279891293016, 1.7142088499342094, 1.713784818413418, 3.240551279383822, 2.476002210087155, 0.9488056750794274, 0.9488056751198732, 0.9488056751022729, 1.7154937384640776, 0.9524070011317125, 0.9524070010127722, 0.9521181348839844, 0.9521181347348595, 0.9521181345783829, 0.9521181345673384, 0.9521181345213954, 0.9521181344158587, 0.9521181343120798, 0.9523169147431785, 0.952167129896842, 0.9521671298837583, 0.9521671296853191, 0.9521671295909064, 0.9514002248526321, 0.9512504359880202, 0.9512504359858088, 1.7152131065370797, 2.474289775749749, 2.4757775345321096, 3.235021305739544, 0.9524294749066232, 0.9522796482865301, 0.9520615615338939, 0.951900108470579, 0.5919317254547137, 0.5919317253141979, 0.5919317252750821, 0.5902106864202247, 0.5906124888623605, 0.5906124887178148, 0.5906124886556755, 0.5906124886039097, 0.591296233532425, 0.5902660175252553, 0.5894255649880988, 0.5900137443675288, 0.5881805169712911, 0.11799996487246295, 0.11799996487255701, 0.11799996487204134, 0.11801699233558338, 0.11801699233518519, 0.11800919288320032, 0.11800919288326155, 0.11800919288312017, 0.11800919288280531, 0.11800919288327616, 0.1180091928827048, 0.11800919288248819, 0.11805005017491774, 0.11804297675461199, 0.1180429767359578, 0.11801643209296517, 0.11800935869617343, 0.5870979193901581, 0.11802593673863315, 0.11802593673807275, 0.11802500289167037, 0.11802496706772525, 0.11802443534502505, 0.11802358732128833, 0.11802176307617043, 0.11802052911928732, 0.11801886516325473, 0.11801886515554422, 0.11801814024317354, 0.11801814024259201, 0.1180178930743505], \"Total\": [10.0, 5.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 4.0, 8.0, 4.0, 2.0, 2.0, 2.0, 14.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 5.0, 2.0, 2.0, 2.323817303724719, 2.323817303717128, 2.323817303638071, 2.323814935701342, 2.3238149357183886, 2.323814935714035, 2.3238149357163573, 2.323812567129768, 10.840528020867707, 3.9214662481917775, 3.0854428161323426, 1.9873296445962338, 1.9873296445603228, 1.9873296445145456, 1.9873296444130775, 5.5006473280962425, 3.9030323931731337, 2.2774456238692022, 2.2774456238557663, 2.2774313955066257, 2.2774141107469936, 2.277414110736321, 2.277414110728347, 2.2774141106990173, 2.277399882476352, 2.305414070504067, 2.3054140704985926, 2.3054050997006414, 2.3054050996984734, 2.3054113485564898, 8.275915358685483, 4.608629374267311, 14.585223347339122, 7.7671192251463, 3.067014543809246, 2.2870219950589936, 2.287021995067756, 2.287021995073312, 2.2870219950768855, 2.287019616834056, 2.2870196168356505, 2.287017237966354, 3.0486403074779127, 1.9692282341984702, 5.5006473280962425, 3.8566257164736903, 3.9030323931731337, 14.585223347339122, 2.2590224706898474, 2.259022470684608, 2.2590224706831523, 2.2590224706863498, 2.2590224706852227, 2.259022470679783, 2.2590224706837203, 2.25900675448998, 2.2590067544858945, 7.7671192251463, 2.3054023765715708, 2.3054023765745932, 2.3054113485564898, 2.3054050996984734, 2.3054050997006414, 2.3054140704985926, 2.305414070504067, 5.805715297089749, 8.275915358685483, 10.840528020867707, 3.066996611769567, 3.067014543809246, 2.2310413479744176, 2.2310413480034486, 2.2310413480046263, 3.020625917202106, 3.020631516371374, 5.805715297089749, 4.608629374267311, 1.9401979174357225, 1.9401979175614152, 1.9401979175344701, 3.828568026541597, 2.2590067544858945, 2.25900675448998, 2.2590224706837203, 2.259022470679783, 2.2590224706852227, 2.2590224706831523, 2.2590224706863498, 2.259022470684608, 2.2590224706898474, 2.277399882476352, 2.2774141106990173, 2.277414110736321, 2.2774141107469936, 2.277414110728347, 2.2774313955066257, 2.2774456238557663, 2.2774456238692022, 4.300356676328858, 7.7671192251463, 8.275915358685483, 14.585223347339122, 3.066996611769567, 3.067014543809246, 3.8566257164736903, 3.0486403074779127, 1.9401979174357225, 1.9401979175344701, 1.9401979175614152, 1.9692282341984702, 1.9873296444130775, 1.9873296445145456, 1.9873296445603228, 1.9873296445962338, 2.776483778775267, 4.300356676328858, 5.805715297089749, 8.275915358685483, 10.840528020867707, 2.2310413479744176, 2.2310413480046263, 2.2310413480034486, 2.25900675448998, 2.2590067544858945, 2.2590224706831523, 2.2590224706863498, 2.2590224706837203, 2.259022470679783, 2.2590224706898474, 2.2590224706852227, 2.259022470684608, 2.2774313955066257, 2.2774456238557663, 2.2774456238692022, 2.277399882476352, 2.2774141106990173, 14.585223347339122, 2.3054050997006414, 2.3054050996984734, 3.828568026541597, 3.066996611769567, 7.7671192251463, 3.020631516371374, 3.9214662481917775, 3.8566257164736903, 2.3054140704985926, 2.305414070504067, 2.3054023765745932, 2.3054023765715708, 3.067014543809246], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"]}, \"token.table\": {\"Term\": [\"2007\", \"2008\", \"2008\", \"2008\", \"ability\", \"ability\", \"ability\", \"adopted\", \"algorithm\", \"algorithm\", \"also\", \"also\", \"although\", \"astle\", \"astle\", \"attention\", \"attention\", \"benefit\", \"benefit\", \"certain\", \"cognitive\", \"cognitive\", \"cognitive\", \"construct\", \"could\", \"data\", \"data\", \"different\", \"effect\", \"effect\", \"effect\", \"evidence\", \"evidence\", \"feature\", \"feature\", \"field\", \"field\", \"first\", \"first\", \"following\", \"following\", \"framework\", \"framework\", \"framework\", \"gathercole\", \"gathercole\", \"group\", \"group\", \"group\", \"holmes\", \"holmes\", \"hulme2013\", \"hulme2013\", \"hypothesis\", \"hypothesis\", \"intelligence\", \"intelligence\", \"intensive\", \"intensive\", \"interest\", \"interest\", \"issue\", \"jaeggi\", \"jaeggi\", \"klingberg\", \"klingberg\", \"make\", \"make\", \"measure\", \"measure\", \"melbylervag\", \"melbylervag\", \"model\", \"model\", \"nback\", \"one\", \"outcome\", \"outcome\", \"outcome\", \"participant\", \"popular\", \"population\", \"procedure\", \"procedure\", \"provide\", \"reading\", \"reading\", \"reliable\", \"reliable\", \"reported\", \"reported\", \"researcher\", \"researcher\", \"result\", \"result\", \"set\", \"set\", \"shared\", \"shared\", \"size\", \"small\", \"study\", \"study\", \"study\", \"study\", \"subprocesses\", \"subprocesses\", \"task\", \"task\", \"task\", \"test\", \"test\", \"testing\", \"trained\", \"trained\", \"training\", \"training\", \"training\", \"training\", \"transfer\", \"transfer\", \"transfer\", \"transfer\", \"two\", \"two\", \"untrained\", \"untrained\", \"use\", \"use\", \"well\", \"well\", \"wm\", \"wm\", \"would\"], \"Topic\": [2, 1, 2, 3, 1, 2, 3, 1, 3, 4, 2, 4, 3, 2, 3, 2, 3, 2, 3, 2, 1, 2, 3, 1, 1, 3, 4, 1, 2, 3, 4, 2, 3, 1, 4, 2, 3, 1, 3, 1, 2, 1, 2, 4, 1, 2, 1, 2, 3, 1, 3, 2, 3, 1, 2, 1, 3, 2, 3, 1, 2, 1, 1, 3, 2, 3, 1, 4, 1, 3, 2, 3, 3, 4, 1, 1, 1, 2, 3, 1, 2, 2, 1, 3, 2, 2, 3, 2, 3, 1, 3, 1, 2, 1, 3, 1, 2, 1, 2, 3, 3, 1, 2, 3, 4, 1, 2, 1, 2, 4, 1, 4, 2, 1, 2, 1, 2, 3, 4, 1, 2, 3, 4, 1, 3, 1, 4, 1, 3, 2, 3, 1, 2, 2], \"Freq\": [0.8744996787562109, 0.2611942619453245, 0.2611942619453245, 0.522388523890649, 0.3260499700004668, 0.3260499700004668, 0.3260499700004668, 0.8606537333333149, 0.5154113356237686, 0.5154113356237686, 0.5078131537185823, 0.5078131537185823, 0.8964423728810733, 0.4426693461336726, 0.4426693461336726, 0.44266934613389347, 0.44266934613389347, 0.44266934613418785, 0.44266934613418785, 0.8745005881354115, 0.2574957255097791, 0.3862435882646687, 0.2574957255097791, 0.8606528563128908, 0.8606537333388761, 0.5154113355903784, 0.5154113355903784, 0.8606528563449819, 0.3444881289653571, 0.5167321934480357, 0.17224406448267854, 0.4426693461340139, 0.4426693461340139, 0.5031877840756168, 0.5031877840756168, 0.656030163707494, 0.328015081853747, 0.43909116295358047, 0.43909116295358047, 0.43376207054161503, 0.43376207054161503, 0.36016778042950043, 0.36016778042950043, 0.36016778042950043, 0.4337637586229647, 0.4337637586229647, 0.32605187634134014, 0.32605187634134014, 0.32605187634134014, 0.43909723878295837, 0.43909723878295837, 0.4426693461329872, 0.4426693461329872, 0.43376324626452467, 0.43376324626452467, 0.43909449550791857, 0.43909449550791857, 0.4426693461349594, 0.4426693461349594, 0.5124220858372165, 0.5124220858372165, 0.8606537333341749, 0.4390944954986685, 0.4390944954986685, 0.4426724258412323, 0.4426724258412323, 0.5031877840499253, 0.5031877840499253, 0.6482051748108674, 0.3241025874054337, 0.33105721377318476, 0.6621144275463695, 0.5154113355975364, 0.5154113355975364, 0.8606528563157023, 0.8606546105696806, 0.2592940237183169, 0.5185880474366338, 0.2592940237183169, 0.8606537333325626, 0.8744996787583355, 0.8744996787548445, 0.43908841972748314, 0.43908841972748314, 0.8745005881360212, 0.44266934613429915, 0.44266934613429915, 0.4426724258404317, 0.4426724258404317, 0.43909449550072627, 0.43909449550072627, 0.433763758622396, 0.433763758622396, 0.4339685050759726, 0.4339685050759726, 0.4337615584100819, 0.4337615584100819, 0.4337632462641168, 0.4337632462641168, 0.8964423728927382, 0.8964423728806001, 0.23253885090612605, 0.23253885090612605, 0.4650777018122521, 0.23253885090612605, 0.43376155840905195, 0.43376155840905195, 0.7379714331811356, 0.1844928582952839, 0.09224642914764195, 0.5031877840383346, 0.5031877840383346, 0.8745014977580259, 0.7650199721553963, 0.2550066573851321, 0.2742501712001377, 0.479937799600241, 0.2056876284001033, 0.06856254280003443, 0.3624976658142755, 0.24166511054285034, 0.24166511054285034, 0.12083255527142517, 0.43908841973007356, 0.43908841973007356, 0.503187784029242, 0.503187784029242, 0.43909449550226365, 0.43909449550226365, 0.3310566001116484, 0.6621132002232968, 0.5453903551817584, 0.5453903551817584, 0.8744996787616859]}, \"lambda.step\": 0.01, \"topic.order\": [1, 3, 2, 4]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el617915012180328465824439\", ldavis_el617915012180328465824439_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el617915012180328465824439\", ldavis_el617915012180328465824439_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el617915012180328465824439\", ldavis_el617915012180328465824439_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "vis = pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary)\n",
    "pyLDAvis.display(vis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does word-word cooccurence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def co_mat(doc):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    import numpy as np\n",
    "    count_model = CountVectorizer(ngram_range=(1,1)) # default unigram model\n",
    "    X = count_model.fit_transform(doc)\n",
    "    Xc = (X.T * X) # this is co-occurrence matrix in sparse csr format\n",
    "    Xc.setdiag(0) # sometimes you want to fill same word cooccurence to 0\n",
    "    print(Xc.todense()) # print out matrix in dense format\n",
    "    return np.asarray(Xc.todense()), count_model.vocabulary_\n",
    "\n",
    "mat, mat_dictionary = co_mat(new_doc)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
